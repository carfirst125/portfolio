## Overview

BERT is conceptually simple and powerful Natural Language Processing model, designed to pretrained deep bidirectional representations from unlabeled text on both left and right context in all layers.

Using BERT in MLAI NLP application accelerates the prototypes to evaluate and launch.

BERT can be used for application such as:

   * **MultiNLI (Multi-Genre Natural Language Inference)**: large dataset. Given a pair of sentences, the goal is to predict whether the second sentence is an entailment, contradiction, or neutral with respect to first one.

   * **QQP (Quora Question Pairs)** is a binary classification tasks. Goal is to determine if two questions asked on Quora are semantically equivalent.
   
   * **QNLI (Question Natural Language Inference)** a version of SQuAD (Standford Question Answering Dataset) which have been converted to a binary classification task. The positive examples: (question, sentence) pairs contains the correct answer. The negative examples: (question, sentence) from the paragraph does not contain the answer.
   
   * **SST-2 (The Stanford Sentiment Treebank)**: a binary single-sentence classification task. Dataset: sentences extracted from movie reviews with human annotations of their sentiment.
   
   * **CoLA (The Corpus of Linguistic Acceptability)**: a binary single-sentence classification task. Dataset: predict grammar of English sentence.
   
   * **STS-B (The Semantic Textual Similarity Benchmark)** a collection of sentence pairs drawn from news headlines and other sources. They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning.
   
   * **MRPC (Microsoft Research Paraphrase Corpus)** sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.
   
   * **RTE (Recognizing Textual Entailment)** a binary entailment task similar to MNLI, but with much less training data.

## Understanding how BERT works

The figure expresses the overview of BERT model architecture. This supports us in understanding how the BERT model is trained, and how to train our BERT model to use.

<img src="https://github.com/carfirst125/portfolio/blob/main/NLP_with_BERT/figures/MLAI_BERT_how_it_works.png" width="90%" height="90%" alt="BERT">

- - - - - 
Feeling free to contact me if you have any questions around.

    Nhan Thanh Ngo (MBA/MSc/BE)
    Email: ngothanhnhan125@gmail.com
    Skype: ngothanhnhan125
    Phone: (+84) 938005052
